{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cabb6ed-e838-4b15-b861-6235df76df41",
   "metadata": {},
   "source": [
    "1. Model Selection:\n",
    "Choose Algorithm: Select an appropriate machine learning algorithm based on the nature of the problem (classification, regression, clustering, etc.), the size of the dataset, and other factors.\n",
    "\n",
    "2. Model Building:\n",
    "Instantiate Model: Create an instance of the chosen machine learning algorithm.\n",
    "\n",
    "Fit Model: Train the model on the training data by calling the fit() method. During training, the model learns the patterns and relationships present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395713c-4f61-4419-897c-f0c501f4cc3c",
   "metadata": {},
   "source": [
    "Random Forest :\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing splitter=\"best\" to the underlying DecisionTreeRegressor. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "\n",
    "Overview:\n",
    "Ensemble Learning: Random Forest is an ensemble learning method, meaning it combines the predictions of multiple individual models (decision trees) to produce a more accurate and robust final prediction.\n",
    "\n",
    "Decision Trees: Random Forest is built upon a collection of decision trees. Each decision tree is trained independently on a random subset of the training data and features.\n",
    "\n",
    "Bagging (Bootstrap Aggregating): Random Forest employs bagging, a technique that involves training each decision tree on a random bootstrap sample (sampling with replacement) from the original dataset. This helps in reducing variance and overfitting.\n",
    "\n",
    "Random Feature Selection: At each node of the decision tree, a random subset of features is considered for splitting, rather than using all features. This further adds randomness to the model and helps in decorrelating the trees.\n",
    "\n",
    "How it works:\n",
    "Training Phase:\n",
    "\n",
    "Random Forest builds a specified number of decision trees (controlled by the n_estimators parameter) using the bootstrapped samples of the training data.\n",
    "At each node of each tree, a random subset of features (controlled by the max_features parameter) is considered for splitting.\n",
    "The trees grow deep enough to minimize impurity (e.g., Gini impurity for classification) or maximize information gain until a stopping criterion is met (e.g., maximum depth of the tree, minimum number of samples required to split a node).\n",
    "Prediction Phase:\n",
    "\n",
    "During prediction, each decision tree in the forest independently classifies the input data point.\n",
    "For classification tasks, the final prediction is typically made by a majority vote (mode) of the predictions of individual trees. For regression tasks, it's the average of the predictions.\n",
    "Key Advantages:\n",
    "Robust to Overfitting: Random Forest is less prone to overfitting compared to individual decision trees, especially when trained with a large number of trees.\n",
    "\n",
    "Handles High-Dimensional Data: It performs well even with a large number of input features.\n",
    "\n",
    "Implicit Feature Selection: By considering only a random subset of features at each split, Random Forest implicitly performs feature selection and can handle irrelevant or redundant features.\n",
    "\n",
    "Scalability: It can efficiently handle large datasets and is highly parallelizable, making it suitable for distributed computing environments.\n",
    "\n",
    "Works Well Out-of-the-Box: Random Forest typically requires minimal hyperparameter tuning and is known for producing good results with default settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c803a6-38af-4437-83ed-cbba23e72db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Library [Details are available in README.md file]\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a448d6-b700-4073-83a2-ff4348e13f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Directory: E:\\upgrade_capston_project-main\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Print the parent directory\n",
    "print(\"Parent Directory:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfb8521-eca1-40b5-bef4-03c0677187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir = parent_dir+'/datasets/processed_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb1ff27-019b-4c2b-82a6-b5d9d693e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the preprocessed data\n",
    "with open(os.path.join(preprocessed_data_dir,'X_train.pkl'), 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "# Load y_train from file\n",
    "with open(os.path.join(preprocessed_data_dir,'y_train.pkl'), 'rb') as f:\n",
    "    y_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9031f07e-8ce5-4582-9bb0-d08e92144811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a Random Forest classifier\n",
    "def train_random_forest(X_train, y_train):\n",
    "\trf_classifier = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=5)\n",
    "\trf_classifier.fit(X_train, y_train)\n",
    "\treturn rf_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40082c60-51c9-48f0-bad8-28175f46eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest classifiers\n",
    "rf_classifier = train_random_forest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbca17c-ad8f-42e2-951f-009aab02e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = parent_dir+'/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b8f8ac-4ea6-4d7a-9f22-dadbc42c953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open(os.path.join(model_dir,'randomForestClassifier.pkl'), 'wb') as f:\n",
    "    pickle.dump(rf_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae16b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
