{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cabb6ed-e838-4b15-b861-6235df76df41",
   "metadata": {},
   "source": [
    "1. Model Selection:\n",
    "Choose Algorithm: Select an appropriate machine learning algorithm based on the nature of the problem (classification, regression, clustering, etc.), the size of the dataset, and other factors.\n",
    "\n",
    "2. Model Building:\n",
    "Instantiate Model: Create an instance of the chosen machine learning algorithm.\n",
    "\n",
    "Fit Model: Train the model on the training data by calling the fit() method. During training, the model learns the patterns and relationships present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395713c-4f61-4419-897c-f0c501f4cc3c",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple yet powerful supervised machine learning algorithm used for both classification and regression tasks. It's a non-parametric, lazy learning algorithm that makes predictions based on the similarity of input data points to the training instances. Here's a detailed description of KNN:\n",
    "\n",
    "1. Algorithm Overview:\n",
    "Lazy Learning: KNN is considered a lazy learning algorithm because it doesn't explicitly build a model during the training phase. Instead, it memorizes the training instances and makes predictions based on their proximity to new, unseen instances during the inference phase.\n",
    "\n",
    "Instance-Based Learning: KNN belongs to the family of instance-based learning algorithms, where predictions are made by comparing new instances to existing instances in the training data.\n",
    "\n",
    "2. How KNN Works:\n",
    "K-Nearest Neighbors: The \"K\" in KNN refers to the number of nearest neighbors (training instances) that are considered when making predictions for a new instance. The value of K is a hyperparameter that needs to be specified by the user.\n",
    "\n",
    "Distance Metric: KNN typically uses distance metrics such as Euclidean distance, Manhattan distance, or Minkowski distance to measure the similarity between data points. Euclidean distance is commonly used for continuous features, while other distance metrics may be preferred for categorical or mixed-type features.\n",
    "\n",
    "Prediction Process: To make predictions for a new instance, KNN calculates the distances between the new instance and all training instances. It then selects the K nearest neighbors based on these distances and assigns the majority class label (in classification) or average target value (in regression) of these neighbors as the predicted label or value for the new instance.\n",
    "\n",
    "3. Key Hyperparameters:\n",
    "K: The number of nearest neighbors to consider when making predictions. The optimal value of K can significantly impact the performance of the KNN algorithm and should be chosen carefully through hyperparameter tuning.\n",
    "\n",
    "Distance Metric: The choice of distance metric (e.g., Euclidean distance, Manhattan distance) can affect the algorithm's performance, especially when dealing with high-dimensional or mixed-type data.\n",
    "\n",
    "4. Advantages of KNN:\n",
    "Simplicity: KNN is easy to understand and implement, making it suitable for beginners and as a baseline model for comparison.\n",
    "\n",
    "No Training Phase: Since KNN is a lazy learning algorithm, there's no explicit training phase, which makes it computationally efficient for large datasets.\n",
    "\n",
    "Versatility: KNN can be applied to both classification and regression tasks and can handle data with mixed types of features.\n",
    "\n",
    "Non-Parametric: KNN makes no assumptions about the underlying data distribution, making it suitable for both linear and non-linear relationships between features and target variables.\n",
    "\n",
    "5. Limitations of KNN:\n",
    "Computational Complexity: KNN requires computing distances between the new instance and all training instances, which can be computationally expensive, especially for large datasets or high-dimensional feature spaces.\n",
    "\n",
    "Sensitivity to Feature Scaling: KNN is sensitive to the scale of features, so it's important to scale or normalize features before applying the algorithm to ensure that all features contribute equally to the distance computation.\n",
    "\n",
    "Curse of Dimensionality: KNN performance may degrade in high-dimensional feature spaces due to the curse of dimensionality, where the distance between data points becomes less meaningful as the number of dimensions increases.\n",
    "\n",
    "Need for Optimal K: The choice of K can significantly impact the performance of KNN, and selecting an inappropriate value of K may lead to suboptimal results.\n",
    "\n",
    "Overall, KNN is a versatile and intuitive algorithm that can be effective in many scenarios, especially when the underlying data distribution is not well understood or when there are no clear boundaries between classes or clusters. However, it's important to consider its limitations and perform appropriate preprocessing and hyperparameter tuning to ensure optimal performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c803a6-38af-4437-83ed-cbba23e72db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Library [Details are available in README.md file]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a448d6-b700-4073-83a2-ff4348e13f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Directory: E:\\upgrade_capston_project-main\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Print the parent directory\n",
    "print(\"Parent Directory:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfb8521-eca1-40b5-bef4-03c0677187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir = parent_dir+'/datasets/processed_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb1ff27-019b-4c2b-82a6-b5d9d693e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the preprocessed data\n",
    "with open(os.path.join(preprocessed_data_dir,'X_train.pkl'), 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "# Load y_train from file\n",
    "with open(os.path.join(preprocessed_data_dir,'y_train.pkl'), 'rb') as f:\n",
    "    y_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9031f07e-8ce5-4582-9bb0-d08e92144811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a KNN\n",
    "def train_knn_model(X_train, y_train):\n",
    "\tknn_classifier = KNeighborsClassifier(n_neighbors=2)\n",
    "\tknn_classifier.fit(X_train, y_train)\n",
    "\treturn knn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40082c60-51c9-48f0-bad8-28175f46eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN\n",
    "knn_classifier = train_knn_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbca17c-ad8f-42e2-951f-009aab02e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = parent_dir+'/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b8f8ac-4ea6-4d7a-9f22-dadbc42c953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open(os.path.join(model_dir,'knn_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(knn_classifier, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
