{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cabb6ed-e838-4b15-b861-6235df76df41",
   "metadata": {},
   "source": [
    "## 1. Model Selection:\n",
    "Choose Algorithm: Select an appropriate machine learning algorithm based on the nature of the problem (classification, regression, clustering, etc.), the size of the dataset, and other factors.\n",
    "\n",
    "## 2. Model Building:\n",
    "Instantiate Model: Create an instance of the chosen machine learning algorithm.\n",
    "\n",
    "Fit Model: Train the model on the training data by calling the fit() method. During training, the model learns the patterns and relationships present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395713c-4f61-4419-897c-f0c501f4cc3c",
   "metadata": {},
   "source": [
    "## XGBoost (Extreme Gradient Boosting) \n",
    "\n",
    "is an advanced implementation of gradient boosting algorithm designed for efficiency, flexibility, and scalability. It is widely used in machine learning competitions and real-world applications due to its state-of-the-art performance and robustness. Here's a detailed description of XGBoost:\n",
    "\n",
    "## 1. Gradient Boosting Algorithm:\n",
    "Boosting Ensemble Method: XGBoost belongs to the family of boosting ensemble methods, where multiple weak learners (usually decision trees) are trained sequentially, and each subsequent model corrects the errors made by the previous models.\n",
    "\n",
    "Gradient Boosting: XGBoost employs the gradient boosting framework, which optimizes a differentiable loss function by iteratively fitting weak learners to the negative gradient of the loss function.\n",
    "\n",
    "## 2. Key Features of XGBoost:\n",
    "Tree Ensemble Method: XGBoost builds an ensemble of decision trees, known as a gradient boosted decision tree (GBDT), to make predictions. Each tree is added sequentially to the ensemble, and subsequent trees learn from the residuals (errors) of the previous trees.\n",
    "\n",
    "Regularization Techniques: \n",
    "XGBoost integrates various regularization techniques to prevent overfitting, including L1 (Lasso) and L2 (Ridge) regularization on leaf weights, and tree pruning to control tree depth and complexity.\n",
    "\n",
    "Customizable Loss Functions: XGBoost supports customizable loss functions for both regression and classification tasks, allowing users to define their own objectives or use predefined objectives like logistic loss, squared loss, etc.\n",
    "\n",
    "Parallel and Distributed Computing: XGBoost is highly optimized for parallel and distributed computing, leveraging multiple CPU cores and supporting distributed computing frameworks like Apache Hadoop and Apache Spark.\n",
    "\n",
    "Optimized Tree Construction: XGBoost employs a number of optimization techniques to speed up tree construction, including approximate tree learning, column block for parallelization, and out-of-core computing for handling large datasets.\n",
    "\n",
    "## 3. Advantages of XGBoost:\n",
    "High Performance: XGBoost is known for its high prediction accuracy and efficiency, making it suitable for both small and large-scale datasets.\n",
    "\n",
    "Flexibility: XGBoost can handle various types of data and tasks, including classification, regression, and ranking, and supports custom loss functions and evaluation metrics.\n",
    "\n",
    "Feature Importance: XGBoost provides built-in feature importance scores, which help in feature selection and understanding the relative importance of input features in making predictions.\n",
    "\n",
    "Robustness: XGBoost is robust to overfitting and can handle noisy data and missing values effectively, thanks to its regularization techniques and handling of missing values during tree construction.\n",
    "\n",
    "## 4. Limitations of XGBoost:\n",
    "Parameter Tuning: XGBoost requires careful parameter tuning, especially for hyperparameters like learning rate, tree depth, and regularization parameters, to achieve optimal performance.\n",
    "\n",
    "Computationally Intensive: Training an XGBoost model can be computationally intensive, especially for large datasets or deep trees, requiring substantial computational resources.\n",
    "\n",
    "Interpretability: While XGBoost provides feature importance scores, the resulting models may not be as interpretable as simpler models like decision trees or linear models.\n",
    "\n",
    "Overall, XGBoost is a powerful and versatile algorithm that excels in a wide range of machine learning tasks. With its robustness, efficiency, and flexibility, XGBoost has become a popular choice for both practitioners and researchers in the field of machine learning and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c803a6-38af-4437-83ed-cbba23e72db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Library [Details are available in README.md file]\n",
    "import xgboost  as xgb\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a448d6-b700-4073-83a2-ff4348e13f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Directory: E:\\upgrade_capston_project-main\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Print the parent directory\n",
    "print(\"Parent Directory:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfb8521-eca1-40b5-bef4-03c0677187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir = parent_dir+'/datasets/processed_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb1ff27-019b-4c2b-82a6-b5d9d693e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the preprocessed data\n",
    "with open(os.path.join(preprocessed_data_dir,'X_train.pkl'), 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "# Load y_train from file\n",
    "with open(os.path.join(preprocessed_data_dir,'y_train.pkl'), 'rb') as f:\n",
    "    y_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9031f07e-8ce5-4582-9bb0-d08e92144811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a XGB\n",
    "def train_xgb_model(X_train, y_train):\n",
    "\txgb_classifier = xgb.XGBClassifier()\n",
    "\txgb_classifier.fit(X_train, y_train)\n",
    "\treturn xgb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af2c4ef-bedd-40b6-9a92-c5e78f6308a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all occurrences defective as 0 and good as 1\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]=='defective' : \n",
    "        y_train[i] = 0\n",
    "    else:\n",
    "        y_train[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40082c60-51c9-48f0-bad8-28175f46eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest classifiers\n",
    "xgb_classifier = train_xgb_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfbca17c-ad8f-42e2-951f-009aab02e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = parent_dir+'/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b8f8ac-4ea6-4d7a-9f22-dadbc42c953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open(os.path.join(model_dir,'xgbClassifier.pkl'), 'wb') as f:\n",
    "    pickle.dump(xgb_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc8cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
