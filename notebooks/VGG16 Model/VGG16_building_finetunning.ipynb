{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caaa2283-d945-4b47-a381-c8a535971aa4",
   "metadata": {},
   "source": [
    "## VGG16 :\n",
    "\n",
    "is a convolutional neural network architecture that was introduced by the Visual Geometry Group (VGG) at the University of Oxford. It is known for its simplicity and effectiveness in image classification tasks. Here's an overview of VGG16:\n",
    "\n",
    "## Architecture: \n",
    "VGG16 consists of 16 convolutional layers, hence the name \"VGG16\". The architecture is characterized by its repeated use of 3x3 convolutional filters, followed by max-pooling layers. There are also a few fully connected layers at the end of the network.\n",
    "\n",
    "## Convolutional Layers:\n",
    "The convolutional layers in VGG16 consist of stacks of 3x3 convolutional filters with a stride of 1, followed by rectified linear activation (ReLU) functions. The number of filters increases as the spatial resolution decreases, leading to a gradual reduction in spatial dimensions and an increase in the number of feature maps.\n",
    "\n",
    "## Max-Pooling Layers: \n",
    "After every two convolutional layers, VGG16 uses max-pooling layers with a 2x2 window and a stride of 2. Max-pooling reduces the spatial dimensions of the feature maps while retaining the most salient features.\n",
    "\n",
    "## Fully Connected Layers:\n",
    "The final layers of VGG16 consist of fully connected layers followed by softmax activation for classification. These layers take the flattened output of the last convolutional layer and transform it into predictions for each class in the classification task.\n",
    "\n",
    "## Pre-Trained Models: \n",
    "VGG16 is often used as a pre-trained model for transfer learning. Pre-trained versions of VGG16, trained on large-scale image datasets like ImageNet, are available in frameworks like TensorFlow and Keras. These pre-trained models have learned rich feature representations from the original dataset and can be fine-tuned or used as feature extractors for other tasks.\n",
    "\n",
    "## Applications: \n",
    "VGG16 has been widely used in various computer vision tasks, including image classification, object detection, and image segmentation. Its relatively simple architecture and strong performance make it a popular choice for baseline experiments and as a backbone in more complex models.\n",
    "\n",
    "While VGG16 achieved impressive results when it was introduced, newer architectures like ResNet, Inception, and EfficientNet have since surpassed it in terms of accuracy and computational efficiency. However, VGG16 remains a valuable tool for understanding and experimenting with deep learning in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013318a7-f5a9-4367-94c8-3f686b020cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libarary\n",
    "from tensorflow.keras import datasets , layers , models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import save_model,Sequential\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc350a77-4c79-406f-94ab-17faaf79f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Print the parent directory\n",
    "print(\"Parent Directory:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85c531-e040-40a4-b3a5-9afe229d308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = parent_dir+\"/datasets/raw_dataset/Digital images of defective and good condition tyres\" # dataset directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be19695-14cb-4529-9ffe-3a1464c93edf",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING and FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83218aa3-c5dc-4438-917e-ba70fa7b862e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for preprocessing\n",
    "batch_size = 32\n",
    "image_size = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27034f04-c4a5-4f3a-b927-744989b7c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data with the help of the tensorflow keras utils model :\n",
    "dataset =  tf.keras.utils.image_dataset_from_directory(dataset_dir ,image_size=image_size)                                                                                 \n",
    "class_name = dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6426ce4-fe0b-4a00-ade8-13e3f095b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augumenation\n",
    "data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
    "                                         tf.keras.layers.RandomRotation(0.2),\n",
    "                                         tf.keras.layers.RandomZoom(0.2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ea4c2-1a43-4d3a-bcde-fdc8aad4f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the object of mobilenet inbuild processess input module\n",
    "preprocess_input = tf.keras.applications.vgg16.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda5aee-3f66-4d14-aab4-149ad68449ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in to train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d94de-89a0-431d-bd2c-0112130134cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(dataset)\n",
    "print(\"%d\"%val_batches)\n",
    "validation_dataset = dataset.take(val_batches//4)\n",
    "train_dataset = dataset.skip(val_batches//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336f338-7e52-431f-a8f2-e5a14dbc07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit the dataset into test and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fefe3-07d3-4d80-bfb4-2ed4a97d899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "print(\"%d\"%val_batches)\n",
    "\n",
    "test_dataset = validation_dataset.take(val_batches//2)\n",
    "validation_dataset = validation_dataset.skip(val_batches//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15522bcb-6aac-4154-9c29-9c6f473ac9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autotune : In TensorFlow, tf.data.AUTOTUNE is a special constant that can be used when configuring input pipelines for better \n",
    "## performance. It's particularly useful when dealing with input data pipelines that involve I/O operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf3614-f834-4f61-aef2-d63bd70f5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE =  tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset =  validation_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8f691-d9dd-414c-8c20-239ec8a113e2",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b01a66-8f70-436e-8e0c-2a4da5898a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model as mobilenetv2\n",
    "\n",
    "IMG_SHAPE = (160,160)+(3,)\n",
    "base_model = tf.keras.applications.VGG16(input_shape = IMG_SHAPE,include_top=False,weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69edce1-4432-4f29-bcc3-2ca0c5a2fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all the train dataset to basemodel and prepare the batch\n",
    "image_batch , label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e42ca-f884-446a-9639-29afd72a6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting base_model.trainable = False means that freezing the weights of the pre-trained model (base_model) during training. \n",
    "#In other words, the model's weights will not be updated during the training process when this flag is set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5170f-2983-40a7-bb2a-5928685847e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d63cd-8b42-40f0-8d27-0799d5a4935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b8991d-167d-4188-b407-306ff48e5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define average pooling layer for feature pooling and dimensionality reduction\n",
    "global_average_layer =  tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8b3ed-eb40-48a8-9d44-93bd7ed95467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction layer with 1 neuron as output of binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c7904-380f-485e-99b1-a52d47823189",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_batch =prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59babd-97d8-4533-9c73-81132256e419",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9177d5fa-3dfe-44ce-8fe8-89c6dce23d26",
   "metadata": {},
   "source": [
    "# Description on below steps:\n",
    "1. inputs - defining the shapr of input to pass to preprocess.\n",
    "2. preprocess_input is the object of mobilenetv2 inbuild prerocessing module to process the data\n",
    "3. pass the input to base model to extract the features for training and the training set to false to maintaine the same weight\n",
    "4. gloabal average pooling layer to do average pool from filter to reduce the dimenationality \n",
    "5. Dropout the 0.2 to drop the noise to get rid from overfitting issue\n",
    "6. output layer is the prediction layer which gave us 1 nuron \n",
    "7 passing input and output to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb382e09-a416-473a-8749-4c75baf7b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(image_size+(3,)))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)         \n",
    "x = base_model(x,training = False)\n",
    "x=  global_average_layer(x)\n",
    "x=  tf.keras.layers.Dropout(0.2)(x)\n",
    "output=prediction_layer(x)\n",
    "models = tf.keras.Model(inputs,output)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c21d1754-09a0-4475-9144-b67bdc17e49d",
   "metadata": {},
   "source": [
    "# Complie the model\n",
    "optimizer =  Adam optimier with learning rate 0.0001 \n",
    "    Adam algorithm to update the model's parameters during training, with a relatively small step size determined by the learning rate. This      setting aims to strike a balance between training stability and convergence speed, but it may require adjustment based on the specific        requirements of the task and the characteristics of the data.\n",
    "loss  = BinaryCrossentropy\n",
    "    BinaryCrossentropy provides an effective way to quantify the difference between the true labels and the model's predictions in binary         classification tasks, helping guide the training process towards better performance.\n",
    "metrics = accuracy\n",
    "     allows you to track the accuracy of your model during training and evaluation, providing valuable insights into its performance on the        classification task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132103e7-d956-4438-80a2-747f0366f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "              ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc432f05-6632-4eef-a814-318924c24af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef225747-bf7b-4603-b442-56c8d02e74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bad16-5674-4954-87c6-a84366a0b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy before training\n",
    "loss0,accuracy0= models.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c68daf-3571-4544-97f9-d48dd8b8f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2b0f5-5bbc-4c17-94fc-6567e02ae43b",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a76a5b-3e1b-4f54-ace6-1a71a6dde38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_epochs = 5\n",
    "history = models.fit(train_dataset,epochs=intial_epochs,validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438cac9f-250e-47b3-b1ba-009d3c4d708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy of trained model\n",
    "plt.plot(history.history['accuracy'],label= 'accuracy')\n",
    "plt.plot(history.history['val_accuracy'],label= 'val_accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"VGG16_accuracy_plot\")\n",
    "plt.savefig(parent_dir+'\\\\visuals\\\\vgg16_accuracy_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179aecf-dd03-40db-9aa3-707bc53ce648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss of trained model\n",
    "plt.plot(history.history['loss'],label= 'loss')\n",
    "plt.plot(history.history['val_loss'],label= 'val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(\"vgg16_loss_plot\")\n",
    "plt.savefig(parent_dir+'\\\\visuals\\\\vgg16_loss_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad86658-8fc0-471e-8afa-48de1a58de5b",
   "metadata": {},
   "source": [
    "## Fine Tunning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a251617-ed17-4dd5-aec9-a77d9c7953ad",
   "metadata": {},
   "source": [
    "Fine-Tuning: When trainable is set to True, the weights of the pre-trained layers in the base_model are unfrozen, allowing them to be modified during training. This enables the model to fine-tune its learned representations to better fit the new task or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ad802-a27f-46b8-b5bd-db81b4a8b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27972d22-f50e-4837-ab77-d36c3885182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of layers in base models\" , len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfa688-90e1-43ce-80fa-e7aa2a9c7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation with RMSprop optimizer this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f157df06-d569-4567-8253-d2650400550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001/10),loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "              ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e472ce8-305e-4cf5-9779-05aa404d5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0a3fd-eed2-4e1a-bfc1-7b51a285428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start model traing on base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e47c09-865c-4763-a4cf-2178c28749ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epoch = 5\n",
    "totol_epoch =  fine_tune_epoch+intial_epochs\n",
    "\n",
    "history = models.fit(train_dataset,epochs=totol_epoch,initial_epoch=history.epoch[-1],validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e7f1e-5f61-4d10-b11e-3a8eb60075c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy of trained model\n",
    "plt.plot(history.history['accuracy'],label= 'accuracy')\n",
    "plt.plot(history.history['val_accuracy'],label= 'val_accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"vgg16_accuracy_plot\")\n",
    "plt.savefig(parent_dir+'\\\\visuals\\\\vgg16_finetune_accuracy_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150f8c2-7117-4f0f-bcad-724bcd888997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss of trained model\n",
    "plt.plot(history.history['loss'],label= 'loss')\n",
    "plt.plot(history.history['val_loss'],label= 'val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(\"vgg16_loss_plot\")\n",
    "plt.savefig(parent_dir+'\\\\visuals\\\\vgg16_finetune_loss_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed8843-3a0b-47c5-b812-efa03adb3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data in form of tensorflow object\n",
    "tf.data.Dataset.save(test_dataset, parent_dir+'/datasets/processed_dataset/vgg16_test_datasets.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651c6a8-fabb-4e78-acc0-d85c051abef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(models , parent_dir+\"/models/vgg16_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946367bb-5029-43dc-b6ac-c2b131711542",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(parent_dir+'/models/vgg16_training_history.pkl', 'wb') as file:\n",
    "    pickle.dump(history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98717705-3a40-4945-a111-42ecdebed032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
