{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d306558-350a-4a7c-88b0-026587615143",
   "metadata": {},
   "source": [
    "# Implement a feedback loop to update the model periodically with fresh manufacturing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3649e04e-f191-45b2-bc2f-31f880de8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libarary\n",
    "from tensorflow.keras import datasets , layers , models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import save_model,Sequential,load_model\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa923f2b-12d5-46b5-bfdc-def0e76e17f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Directory: C:\\Users\\sonu.a.jain\\AppData\\Local\\miniconda3\\Scripts\\faultyFinding\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Print the parent directory\n",
    "print(\"Parent Directory:\", parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c62e8-98b2-49c8-af33-000622dc1266",
   "metadata": {},
   "source": [
    "## Add the data in feedbackloopdata folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc02ad-af4e-43a9-a435-5f3d93151d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = parent_dir+'/datasets/feedbackLoopData' # dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04380805-b3af-41da-9bde-1e48e853dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for preprocessing\n",
    "batch_size = 32\n",
    "image_size = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8f66d-7580-43cf-96ea-31f666181db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data with the help of the tensorflow keras utils model :\n",
    "dataset =  tf.keras.utils.image_dataset_from_directory(dataset_dir ,image_size=image_size)                                                                                 \n",
    "class_name = dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5331217-54b3-4708-9099-5337bba47f39",
   "metadata": {},
   "source": [
    "## Split the dataset in to train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8251457-c3cc-4573-8298-22138fc5eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(dataset)\n",
    "print(\"%d\"%val_batches)\n",
    "validation_dataset = dataset.take(val_batches//4)\n",
    "train_dataset = dataset.skip(val_batches//4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290519f-c1dd-4f17-95e1-1e752e8dca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit the dataset into test and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223c8a0-68f1-42ee-bae9-2923be89a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "print(\"%d\"%val_batches)\n",
    "\n",
    "test_dataset = validation_dataset.take(val_batches//2)\n",
    "validation_dataset = validation_dataset.skip(val_batches//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace705e8-b329-4ce3-b5fe-1dcb86521b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autotune : In TensorFlow, tf.data.AUTOTUNE is a special constant that can be used when configuring input pipelines for better \n",
    "## performance. It's particularly useful when dealing with input data pipelines that involve I/O operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a0f7c-081d-4239-bf24-99bcdcc85d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE =  tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset =  validation_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117679ee-611c-4f4c-b9ad-0f9c7da067eb",
   "metadata": {},
   "source": [
    "## Laod the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb50fb7-21e4-4605-859d-f7403157c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "models = load_model(parent_dir+\"/models/vgg16_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d9e2e33-cfb8-4fb5-919a-93496fec4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the history of pretrained model to identify the epoch count till trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03beb19-6b4b-4a2e-91c3-fd08e648e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.callbacks.history.History object at 0x0000013445DC2660>\n"
     ]
    }
   ],
   "source": [
    "with open(parent_dir+'/models/vgg16_training_history.pkl', 'rb') as file:\n",
    "    loaded_history = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0262e2a-dab0-4288-ae47-b47507fbd47e",
   "metadata": {},
   "source": [
    "## Train with new fresh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0ec49-e999-4be3-b044-81bc406a9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epoch = 5\n",
    "initial_epoch=len(loaded_history.epoch)\n",
    "totol_epoch =  fine_tune_epoch+intial_epochs\n",
    "history = models.fit(train_dataset,epochs=totol_epoch,initial_epoch=history.epoch[-1],validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b8202-a083-47d8-9d9f-858ccf0596dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy of trained model\n",
    "plt.plot(history.history['accuracy'],label= 'accuracy')\n",
    "plt.plot(history.history['val_accuracy'],label= 'val_accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"vgg16_accuracy_plot\")\n",
    "plt.savefig(parent_dir+'\\\\visuals\\\\vgg16_finetune_accuracy_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728f67e-b6c2-4153-a54f-f8dca3f089b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss of trained model\n",
    "plt.plot(history.history['loss'],label= 'loss')\n",
    "plt.plot(history.history['val_loss'],label= 'val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(\"vgg16_loss_plot\")\n",
    "plt.savefig(parent_dir+'\\\\visuals\\\\vgg16_finetune_loss_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67083ade-2818-4408-b4b1-c1c80cfda9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (Keeping the file name same so that the model can always retain the history of past trained data)\n",
    "save_model(models , parent_dir+\"/models/vgg16_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c46d05-6ed8-4273-9bfe-6c9f8dac5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (Keeping the file name same so that the history can always retain the history of past trained data)\n",
    "with open(parent_dir+'/models/vgg16_training_history.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a2949a-b0d6-430e-a349-f271bbab2d2f",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60976921-5595-4feb-9d41-c8cbf3a88476",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss , accuracy = models.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
