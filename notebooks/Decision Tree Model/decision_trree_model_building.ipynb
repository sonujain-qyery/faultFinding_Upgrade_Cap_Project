{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cabb6ed-e838-4b15-b861-6235df76df41",
   "metadata": {},
   "source": [
    "1. Model Selection:\n",
    "Choose Algorithm: Select an appropriate machine learning algorithm based on the nature of the problem (classification, regression, clustering, etc.), the size of the dataset, and other factors.\n",
    "\n",
    "2. Model Building:\n",
    "Instantiate Model: Create an instance of the chosen machine learning algorithm.\n",
    "\n",
    "Fit Model: Train the model on the training data by calling the fit() method. During training, the model learns the patterns and relationships present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395713c-4f61-4419-897c-f0c501f4cc3c",
   "metadata": {},
   "source": [
    "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It builds a tree-like structure where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents the class label (in classification) or predicted value (in regression). Here's a description of the Decision Tree model:\n",
    "\n",
    "1. Structure of Decision Tree:\n",
    "Root Node: The topmost node in the tree, which corresponds to the feature that best splits the data into homogeneous subsets based on a certain criterion (e.g., Gini impurity, entropy).\n",
    "\n",
    "Internal Nodes: Intermediate nodes in the tree that represent decisions based on feature values. Each internal node tests a specific feature and splits the data into subsets accordingly.\n",
    "\n",
    "Leaf Nodes: Terminal nodes in the tree that represent the final prediction or classification label. Each leaf node corresponds to a class label (in classification) or predicted value (in regression).\n",
    "\n",
    "2. Training Process:\n",
    "Splitting Criteria: The decision tree algorithm recursively selects the best feature to split the data at each node. It evaluates different splitting criteria (e.g., Gini impurity, entropy) to determine the feature that maximizes the homogeneity of the resulting subsets.\n",
    "\n",
    "Recursive Partitioning: The dataset is recursively partitioned into subsets based on the selected feature and its possible values. This process continues until certain stopping criteria are met, such as maximum tree depth, minimum samples per leaf, or minimum impurity decrease.\n",
    "\n",
    "3. Prediction Process:\n",
    "Traversal: To make predictions for a new instance, it traverses the decision tree from the root node to a leaf node based on the feature values of the instance.\n",
    "\n",
    "Classification: In classification tasks, the class label associated with the leaf node reached by the instance determines the predicted class.\n",
    "\n",
    "Regression: In regression tasks, the predicted value associated with the leaf node reached by the instance is the final predicted value.\n",
    "\n",
    "4. Key Advantages:\n",
    "Interpretability: Decision Trees are easy to interpret and visualize, making them useful for understanding the underlying decision-making process of the model.\n",
    "\n",
    "Non-Parametric: Decision Trees make no assumptions about the underlying data distribution and can handle both numerical and categorical features.\n",
    "\n",
    "Handles Non-Linear Relationships: Decision Trees can capture complex non-linear relationships between features and the target variable.\n",
    "\n",
    "Feature Importance: Decision Trees can provide information about feature importance, which helps in feature selection and understanding the most influential features.\n",
    "\n",
    "5. Key Limitations:\n",
    "Overfitting: Decision Trees are prone to overfitting, especially when the tree depth is not properly controlled or the training data is noisy.\n",
    "\n",
    "Instability: Small variations in the training data can lead to significantly different tree structures, making the model unstable.\n",
    "\n",
    "Bias towards Features with Many Levels: Decision Trees tend to favor features with many levels (high cardinality) during the splitting process, which can lead to biased trees.\n",
    "\n",
    "Limited Generalization: Decision Trees may not generalize well to unseen data, especially when the decision boundaries are too complex.\n",
    "\n",
    "Overall, Decision Trees are versatile and widely used in various domains due to their simplicity, interpretability, and ability to handle both classification and regression tasks. However, they are often used in ensemble methods (e.g., Random Forests, Gradient Boosting) to overcome their limitations and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c803a6-38af-4437-83ed-cbba23e72db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Library [Details are available in README.md file]\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a448d6-b700-4073-83a2-ff4348e13f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Directory: E:\\upgrade_capston_project-main\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Get the parent directory (one level up)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Print the parent directory\n",
    "print(\"Parent Directory:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfb8521-eca1-40b5-bef4-03c0677187fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir = parent_dir+'/datasets/processed_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb1ff27-019b-4c2b-82a6-b5d9d693e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the preprocessed data\n",
    "with open(os.path.join(preprocessed_data_dir,'X_train.pkl'), 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "# Load y_train from file\n",
    "with open(os.path.join(preprocessed_data_dir,'y_train.pkl'), 'rb') as f:\n",
    "    y_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9031f07e-8ce5-4582-9bb0-d08e92144811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a decision tree\n",
    "def train_decision_tree(X_train, y_train):\n",
    "\tdt_classifier = DecisionTreeClassifier()\n",
    "\tdt_classifier.fit(X_train, y_train)\n",
    "\treturn dt_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40082c60-51c9-48f0-bad8-28175f46eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train decision tree\n",
    "dt_classifier = train_decision_tree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbca17c-ad8f-42e2-951f-009aab02e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = parent_dir+'/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b8f8ac-4ea6-4d7a-9f22-dadbc42c953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open(os.path.join(model_dir,'decisionTreeModel.pkl'), 'wb') as f:\n",
    "    pickle.dump(dt_classifier, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
